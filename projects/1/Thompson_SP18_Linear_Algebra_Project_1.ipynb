{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Explain what and why we did in 7.6 Parameter ESTIMATION--MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SP18: Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tim Thompson\n",
    "### timathom@indiana.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### _Note_: I do not believe that the current OnRamp course, ostensibly focused on linear algebra, provides enough context to adequately complete this project assignment. Advanced concepts in probability theory are not covered in the course, so it is not clear to me--lacking prior knowledge--how to derive the probability density functions involved in MAP estimation. As a result, my answer for this assignment is very brief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 7.6, Parameter Estimation: MAP, from the _Machine Learning with Python_ OnRamp course, the maximum a posteriori (MAP) estimation procedure is introduced in order to estimate the unknown parameters of a probability distribution. In the current course, _Linear Algebra_, MAP is presented as a \"parameterized optimization problem.\" \n",
    "\n",
    "Unlike maximum likelihood estimation, which simplifies to the sample mean, MAP incorporates a prior probability distribution, based on Bayes' Theorem. The prior distribution for $\\theta$ is specified through a set of parameters that define the prior distribution.\n",
    "\n",
    "$$\\theta_{MAP}=\\underset\\theta{argmax}P{({\\theta{}\\vert{}D})}=\\underset\\theta{argmax}\\frac{P{({D{}\\vert{}\\theta})}P{(\\theta)}}{P{(D)}}{\\text{ .}}$$\n",
    "\n",
    "Simplified, the equation can be expressed using logarithms:\n",
    "\n",
    "$$\\begin{array}{rlrlrlrlrlrl}&\\theta_{MAP}=\\underset\\theta{argmax}P{({D{}\\vert{}\\theta})}P{(\\theta)}=\\underset\\theta{argmax}P{(\\theta)}\\prod_{i=1}^nP{({X_i{}\\vert{}\\theta})}\\\\[3pt]&\\log{(\\underset\\theta{argmax}P{({D{}\\vert{}\\theta})}P{(\\theta)})}=\\underset\\theta{argmax}\\log{(P{(\\theta)})}+\\sum_{i=1}^n\\log{(P{({X_i{}\\vert{}\\theta})})}{\\text{ .}}\\end{array}$$\n",
    "\n",
    "Taking the derivative of the MAP function and setting it to $0$ allows us to find the critical point(s) of the function. Taking the second derivative, $P^{\\prime\\prime}(x)$, lets us test whether the critical point is at a local maximum or minimum. \n",
    "\n",
    "In the case of MAP, we are looking for the maximum of the probability density function, which will be the peak of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
